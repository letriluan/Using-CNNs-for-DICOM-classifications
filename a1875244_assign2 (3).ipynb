{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/letriluan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/letriluan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>Marlise Simons</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "      <td>PARIS Islamic State drive ancient city Palmyra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>art</td>\n",
       "      <td>angel everywhere Mu iz family apartment Bronx ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>finally Second Avenue subway open New York Cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>3/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON time Republicans tumultuous decade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Megyn Kelly shift Fox News NBC host daily dayt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>18460</td>\n",
       "      <td>Gerry Mullany</td>\n",
       "      <td>14/03/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>accidents</td>\n",
       "      <td>HONG KONG Hundreds pilot whale swam shallow Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>18461</td>\n",
       "      <td>Rory Smith</td>\n",
       "      <td>10/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>sports</td>\n",
       "      <td>NICE France Riv accept compliment reject compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>18462</td>\n",
       "      <td>Jack Ewing</td>\n",
       "      <td>9/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>business</td>\n",
       "      <td>FRANKFURT Germans never really warm euro may a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>18463</td>\n",
       "      <td>Scott Cacciola</td>\n",
       "      <td>10/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>sports</td>\n",
       "      <td>Charles Oakley strong feeling compe ticket fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>18465</td>\n",
       "      <td>Sam Roberts</td>\n",
       "      <td>10/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>Hans rosle swedish doctor transform statistici...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               author        date  year month         topic  \\\n",
       "0    17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1    17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2    17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3    17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4    17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "..     ...                  ...         ...   ...   ...           ...   \n",
       "995  18460        Gerry Mullany  14/03/2017  2017     3     accidents   \n",
       "996  18461           Rory Smith  10/02/2017  2017     2        sports   \n",
       "997  18462           Jack Ewing   9/02/2017  2017     2      business   \n",
       "998  18463       Scott Cacciola  10/02/2017  2017     2        sports   \n",
       "999  18465          Sam Roberts  10/02/2017  2017     2     lifestyle   \n",
       "\n",
       "                                               article  \n",
       "0    PARIS Islamic State drive ancient city Palmyra...  \n",
       "1    angel everywhere Mu iz family apartment Bronx ...  \n",
       "2    finally Second Avenue subway open New York Cit...  \n",
       "3    WASHINGTON time Republicans tumultuous decade ...  \n",
       "4    Megyn Kelly shift Fox News NBC host daily dayt...  \n",
       "..                                                 ...  \n",
       "995  HONG KONG Hundreds pilot whale swam shallow Ne...  \n",
       "996  NICE France Riv accept compliment reject compa...  \n",
       "997  FRANKFURT Germans never really warm euro may a...  \n",
       "998  Charles Oakley strong feeling compe ticket fre...  \n",
       "999  Hans rosle swedish doctor transform statistici...  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/letriluan/Downloads/NLP/cleaned_data.csv', encoding=\"latin1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       1000 non-null   int64 \n",
      " 1   author   994 non-null    object\n",
      " 2   date     1000 non-null   object\n",
      " 3   year     1000 non-null   object\n",
      " 4   month    1000 non-null   object\n",
      " 5   topic    1000 non-null   object\n",
      " 6   article  1000 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17878.53200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>341.50184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17283.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17582.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17881.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18182.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18465.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id\n",
       "count   1000.00000\n",
       "mean   17878.53200\n",
       "std      341.50184\n",
       "min    17283.00000\n",
       "25%    17582.75000\n",
       "50%    17881.50000\n",
       "75%    18182.25000\n",
       "max    18465.00000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         0\n",
       "author     6\n",
       "date       0\n",
       "year       0\n",
       "month      0\n",
       "topic      0\n",
       "article    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "def clean_text1(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\b\\w+\\b') \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    doc = nlp(\" \".join(filtered_tokens))\n",
    "    lemmatized_tokens = [token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc]\n",
    "    \n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "df_new = df.copy()\n",
    "df_new[\"article\"] = df_new[\"article\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coreference Resolution utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_coreferences(text):\n",
    "    doc = nlp(text)\n",
    "    if doc._.has_coref:\n",
    "        return doc._.coref_resolved\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text matching utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def find_most_relevant_sentence(question, article_text):\n",
    "    doc = nlp(article_text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    question_embedding = model.encode(question)\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "    similarities = util.pytorch_cos_sim(question_embedding, sentence_embeddings).squeeze()\n",
    "    most_similar_index = similarities.argmax().item()\n",
    "    confidence = similarities[most_similar_index].item()\n",
    "    \n",
    "    return sentences[most_similar_index], confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def extract_relevant_snippets(question, relevant_sentence):\n",
    "    doc = nlp(relevant_sentence)\n",
    "    question_doc = nlp(question)\n",
    "\n",
    "    target_label = 'PERSON'  # Default to PERSON\n",
    "    \n",
    "    if any(word in question.lower() for word in ['who', 'name']):\n",
    "        target_label = 'PERSON'\n",
    "    elif any(word in question.lower() for word in ['when', 'date', 'year', 'time']):\n",
    "        target_label = 'DATE'\n",
    "    elif any(word in question.lower() for word in ['where', 'city', 'country', 'place', 'location']):\n",
    "        target_label = 'GPE'\n",
    "    elif any(word in question.lower() for word in ['what', 'company', 'organization']):\n",
    "        target_label = 'ORG'\n",
    "    elif 'how many' in question.lower():\n",
    "        target_label = 'CARDINAL' \n",
    "\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == target_label:\n",
    "            if ent.text in entities:\n",
    "                entities[ent.text] += 1\n",
    "            else:\n",
    "                entities[ent.text] = 1\n",
    "\n",
    "    if entities:\n",
    "        sorted_entities = sorted(entities.items(), key=lambda item: (-item[1], relevant_sentence.index(item[0])))\n",
    "        return sorted_entities[0][0]\n",
    "\n",
    "    return \"No relevant information found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer snippet: ('Jay Lee', 0.5828641653060913)\n"
     ]
    }
   ],
   "source": [
    "def answer_question_from_article(article_id, question, df):\n",
    "    try:\n",
    "        article_text = df.loc[df['id'] == article_id, 'article'].values[0]\n",
    "    except IndexError:\n",
    "        return \"Article not found.\"\n",
    "\n",
    "    relevant_sentence, confidence = find_most_relevant_sentence(question, article_text)\n",
    "    \n",
    "    confidence_threshold = 0.5\n",
    "    if confidence < confidence_threshold:\n",
    "        return \"High confidence answer not found.\"\n",
    "    \n",
    "    answer_snippet = extract_relevant_snippets(question, relevant_sentence)\n",
    "    return answer_snippet, confidence\n",
    "\n",
    "article_id = 17574  \n",
    "question = \"Who is the vice chairman of Samsung?\"\n",
    "answer = answer_question_from_article(article_id, question, df_new)\n",
    "print(\"Answer snippet:\", answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(queries, answers, article_ids, df):\n",
    "    reciprocal_ranks = []\n",
    "    for query, correct_answer, article_id in zip(queries, answers, article_ids):\n",
    "        predicted_answer, _ = answer_question_from_article(article_id, query, df)\n",
    "        if predicted_answer == correct_answer:\n",
    "            reciprocal_ranks.append(1)  # Correct answer was ranked first\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)  # Correct answer was not found\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "\n",
    "def calculate_map(queries, answers, article_ids, df):\n",
    "    average_precisions = []\n",
    "    for query, correct_answer, article_id in zip(queries, answers, article_ids):\n",
    "        predicted_answer, _ = answer_question_from_article(article_id, query, df)\n",
    "        if predicted_answer == correct_answer:\n",
    "            average_precisions.append(1)  # Correct answer, precision of 1\n",
    "        else:\n",
    "            average_precisions.append(0)  # Incorrect answer\n",
    "    return sum(average_precisions) / len(average_precisions) if average_precisions else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR Score: 0.8\n",
      "MAP Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Define a more complete set of test questions and corresponding answers based on your dataset structure\n",
    "questions = [\n",
    "    \"Who is the vice chairman of Samsung?\",\n",
    "    \"When will the vice chairman of Samsung be questioned?\",\n",
    "    \"How many pilot whales that swam into a shallow New Zealand bay died overnight?\",\n",
    "    \"How many rescuers tried frantically to send the pilot whales back out to sea?\",\n",
    "    \"Where is has one of the highest rates of whale strandings?\"\n",
    "]\n",
    "answers = [\n",
    "    \"Jay Lee\",\n",
    "    \"Thursday\",\n",
    "    \"Hunders\",\n",
    "    \"500\",\n",
    "    \"New Zealand\"\n",
    "]\n",
    "article_ids = [\n",
    "    17574,  # Assume all questions pertain to details in the article with ID 17574\n",
    "    17574,\n",
    "    18460,\n",
    "    18460,\n",
    "    18460\n",
    "]\n",
    "\n",
    "# Assuming df_new is properly loaded and structured as per your system requirements\n",
    "mrr_score = calculate_mrr(questions, answers, article_ids, df_new)\n",
    "map_score = calculate_map(questions, answers, article_ids, df_new)\n",
    "\n",
    "print(\"MRR Score:\", mrr_score)\n",
    "print(\"MAP Score:\", map_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  17574\n",
      "Your question:  Who run the Samsung effectively?\n",
      "Answer: High confidence answer not found.\n",
      "Article ID:  17574\n",
      "Your question:  Who is vice the chairman of Samsung?\n",
      "Answer: ('Jay Lee', 0.5779139995574951)\n",
      "Article ID:  17574\n",
      "Your question:  Who is vice the chairman of Samsung?\n",
      "Answer: ('Jay Lee', 0.5779139995574951)\n"
     ]
    }
   ],
   "source": [
    "def user_interaction():\n",
    "    while True:\n",
    "        article_id = input(\"Enter the article ID or type 'quit' to exit: \")\n",
    "        if article_id.lower() == 'quit':\n",
    "            break\n",
    "        print( 'Article ID: ', article_id)\n",
    "        question = input(\"Enter your question: \")\n",
    "\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        print('Your question: ', question)\n",
    "        try:\n",
    "            article_id = int(article_id)\n",
    "            answer = answer_question_from_article(article_id, question, df_new)\n",
    "            print(\"Answer:\", answer)\n",
    "        except ValueError:\n",
    "            print(\"Invalid article ID. Please enter a numeric ID.\")\n",
    "\n",
    "# To run the interaction\n",
    "user_interaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  15\n",
      "Your question:  who I am ?\n",
      "Answer: Article not found.\n",
      "Article ID:  17574\n",
      "Your question:  Who run the Samsung effectively?\n",
      "Answer: High confidence answer not found.\n",
      "Article ID:  17574\n",
      "Your question:  Who is vice the chairman of Samsung?\n",
      "Answer: ('Jay Lee', 0.5779139995574951)\n",
      "Article ID:  17574\n",
      "Your question:  When the vice Chaiman of Samsung will be questioned?\n",
      "Answer: ('Thursday', 0.5326932072639465)\n"
     ]
    }
   ],
   "source": [
    "user_interaction()  #Tranformers based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  17574\n",
      "Your question:  When Jay Lee will be questioned?\n",
      "Answer: ('November', 0.5102343559265137)\n"
     ]
    }
   ],
   "source": [
    "user_interaction()  #Tranformers based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  18460\n",
      "Your question:  How many pilot whales that swam into a shallow New Zealand bay died overnight? \n",
      "Answer: ('Hundreds', 0.8101339936256409)\n",
      "Article ID:  18460\n",
      "Your question:  How many rescuers tried frantically to send the pilot whales back out to sea?\n",
      "Answer: ('500', 0.7037572264671326)\n",
      "Article ID:  18460\n",
      "Your question:  Where is has one of the highest rates of whale strandings?\n",
      "Answer: ('New Zealand', 0.6138548851013184)\n"
     ]
    }
   ],
   "source": [
    "user_interaction() #Tranformers based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer snippet: new zealand\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "def answer_question_bert(question, context):\n",
    "    \"\"\"Function to answer questions using BERT directly from the context.\"\"\"\n",
    "    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs, return_dict=True)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Convert the tokens back to the original words\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "def answer_question_bert(question, context):\n",
    "    \"\"\"Function to answer questions using BERT directly from the context, including confidence score.\"\"\"\n",
    "    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        outputs = model(**inputs)\n",
    "        answer_start_scores = outputs.start_logits\n",
    "        answer_end_scores = outputs.end_logits\n",
    "\n",
    "    start_probs = softmax(answer_start_scores, dim=-1)\n",
    "    end_probs = softmax(answer_end_scores, dim=-1)\n",
    "    answer_start = torch.argmax(start_probs)\n",
    "    answer_end = torch.argmax(end_probs) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    return answer\n",
    "\n",
    "def answer_question_from_article(article_id, question, df):\n",
    "    \"\"\"Retrieve an article by ID and use BERT to answer a question based on the article's text, including confidence.\"\"\"\n",
    "    try:\n",
    "        article_text = df.loc[df['id'] == article_id, 'article'].values[0]\n",
    "    except IndexError:\n",
    "        return \"Article not found.\"\n",
    "\n",
    "    answer = answer_question_bert(question, article_text)\n",
    "    return answer\n",
    "\n",
    "article_id = 18460\n",
    "question = \"Where is has one of the highest rates of whale strandings?\"\n",
    "answer = answer_question_from_article(article_id, question, df_new)\n",
    "print(\"Answer snippet:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  15\n",
      "Your question:  who I am?\n",
      "Answer: Article not found.\n",
      "Article ID:  17574\n",
      "Your question:  Who run the Samsung effectively?\n",
      "Answer: mr lee\n",
      "Article ID:  17574\n",
      "Your question:  Who is vice the chairman of Samsung?\n",
      "Answer: jay lee\n",
      "Article ID:  17574\n",
      "Your question:  When Jay Lee will be questioned? \n",
      "Answer: thursday\n"
     ]
    }
   ],
   "source": [
    "user_interaction() #BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  18460\n",
      "Your question:  How many pilot whales that swam into a shallow New Zealand bay died overnight?\n",
      "Answer: hundreds\n",
      "Article ID:  18460\n",
      "Your question:  How many rescuers tried frantically to send the pilot whales back out to sea?\n",
      "Answer: 500\n",
      "Article ID:  18460\n",
      "Your question:  Where is has one of the highest rates of whale strandings?\n",
      "Answer: new zealand\n"
     ]
    }
   ],
   "source": [
    "user_interaction() #BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "df = pd.read_csv('cleaned_data.csv', encoding=\"latin1\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def find_most_relevant_sentence(question, article_text):\n",
    "    doc = nlp(article_text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    question_embedding = model.encode(question)\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "    similarities = util.pytorch_cos_sim(question_embedding, sentence_embeddings).squeeze()\n",
    "    most_similar_index = similarities.argmax().item()\n",
    "    confidence = similarities[most_similar_index].item()\n",
    "    \n",
    "    return sentences[most_similar_index], confidence\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def extract_relevant_snippets(question, relevant_sentence):\n",
    "    doc = nlp(relevant_sentence)\n",
    "    question_doc = nlp(question)\n",
    "\n",
    "    target_label = 'PERSON'  # Default to PERSON\n",
    "    \n",
    "    if any(word in question.lower() for word in ['who', 'name']):\n",
    "        target_label = 'PERSON'\n",
    "    elif any(word in question.lower() for word in ['when', 'date', 'year', 'time']):\n",
    "        target_label = 'DATE'\n",
    "    elif any(word in question.lower() for word in ['where', 'city', 'country', 'place', 'location']):\n",
    "        target_label = 'GPE'\n",
    "    elif any(word in question.lower() for word in ['what', 'company', 'organization']):\n",
    "        target_label = 'ORG'\n",
    "    elif 'how many' in question.lower():\n",
    "        target_label = 'CARDINAL' \n",
    "\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == target_label:\n",
    "            if ent.text in entities:\n",
    "                entities[ent.text] += 1\n",
    "            else:\n",
    "                entities[ent.text] = 1\n",
    "\n",
    "    if entities:\n",
    "        sorted_entities = sorted(entities.items(), key=lambda item: (-item[1], relevant_sentence.index(item[0])))\n",
    "        return sorted_entities[0][0]\n",
    "\n",
    "    return \"No relevant information found.\"\n",
    "\n",
    "\n",
    "def answer_question_from_article(article_id, question, df):\n",
    "    try:\n",
    "        article_text = df.loc[df['id'] == article_id, 'article'].values[0]\n",
    "    except IndexError:\n",
    "        return \"Article not found.\"\n",
    "\n",
    "    relevant_sentence, confidence = find_most_relevant_sentence(question, article_text)\n",
    "    \n",
    "    confidence_threshold = 0.5\n",
    "    if confidence < confidence_threshold:\n",
    "        return \"High confidence answer not found.\"\n",
    "    \n",
    "    answer_snippet = extract_relevant_snippets(question, relevant_sentence)\n",
    "    return answer_snippet, confidence\n",
    "\n",
    "def main():\n",
    "    st.title(\"Question Answering System\")\n",
    "    question = st.text_input(\"Enter your question:\")\n",
    "    article_id = st.number_input(\"Enter the article ID:\", value=0, step=1)\n",
    "\n",
    "    if st.button(\"Get Answer\"):\n",
    "        answer = answer_question_from_article(article_id, question, df)\n",
    "        st.write(\"Answer:\", answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
